---
layout: about
title: about
permalink: /
subtitle: <b>PhD candidate at the University of Edinburgh, Robotics and Autonomous systems CDT</b>


profile:
  align: right
  image: prof_pic.jpg
  image_circular: true # crops the image to make it circular
  more_info: >
    <p>S[DOT]GARCIN[AT]ED.AC.UK</p>

news: true # includes a list of news item
[//]: # (latest_posts: false # includes a list of the latest posts)
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

I am a PhD candidate and a member of the [Autonomous Agents Research Group](https://agents.inf.ed.ac.uk/) at the University of Edinburgh. I am fortunate to be co-advised by [Stefano Albrecht](https://agents.inf.ed.ac.uk/stefano-albrecht/) and [Chris Lucas](https://lucaslab-uoe.github.io/members/chris/). Before starting my PhD I completed an MEng in Aeronautics at Imperial College London, where I started the [Imperial College Aerial Vehicle project](https://icavproject.wordpress.com), and then worked three years in the aerial robotics industry. I have been previously involved in [multi-agent robotic navigation](https://ieeexplore.ieee.org/abstract/document/9143181) and [autonomous](https://github.com/uoe-agents/IGP2) [vehicles](https://ieeexplore.ieee.org/abstract/document/9636279) research projects.

My PhD investigates how to understand the _zero-shot transfer_ capabilities of deep reinforcement learning algorithms through the study of the _flow of information_ passing through the model over the course of training and its effect on the model's internal representation. Taking an information theoretic perspective to understand generalisation has been effective in supervised learning, however RL possesses certain particularities that make the study of this flow in this space particularly interesting. For instance, RL agents generate their own training data and create a _feedback loop_ in this information flow. This generation process is also a function of the environment instance selected for the current training episode. In [my most recent work](https://arxiv.org/abs/2402.03479), I investigate how assuming control over the environment instantiation process may regularise the information flow and improve transfer. Recently, I have started considering whether a similar perspective may help design new representation learning objectives for RL agents.

I am _always_ interested in collaboration opportunities so feel free to reach out via email or through any of the channels listed at the bottom of this page! I organise our group's virtual [Reinforcement Learning Reading Group](https://agents.inf.ed.ac.uk/reading-group/), do reach out if you would like to come present your work. If you are in Edinburgh, I also organise an in-person discussion group on the topic of generalisation in machine learning. I am currently on the market for research internships.

[//]: # (Finally, the non-stationarity of the training distribution and its interdependence with the learned model challenges the assumptions made in prior work regarding information flow and generalisation, and as such part of this project investigates how to extend existing results to the reinforcement learning setting.)

[//]: # (subtitle: <p style="color:var&#40;--global-theme-color&#41;;">PhD candidate at the University of Edinburgh, Robotics and Autonomous systems CDT</p>)

[//]: # (subtitle: **PhD candidate at the University of Edinburgh, Robotics and Autonomous systems CDT**)
[//]: # (Link to your social media connections, too. This theme is set up to use [Font Awesome icons]&#40;https://fontawesome.com/&#41; and [Academicons]&#40;https://jpswalsh.github.io/academicons/&#41;, like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them.)